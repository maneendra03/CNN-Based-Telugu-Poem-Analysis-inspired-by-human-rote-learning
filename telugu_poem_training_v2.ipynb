{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d166f8e3",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b975d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp -r '/content/drive/MyDrive/majorproject - A' /content/project\n",
    "# %cd /content/project\n",
    "\n",
    "# For local setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061dc326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS (Apple Silicon)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6107b3c8",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Telugu poems\n",
    "data_path = Path('data/processed/telugu_poems.json')\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(\"Dataset not found. Creating...\")\n",
    "    from scripts.create_large_dataset import main as create_dataset\n",
    "    create_dataset()\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    poems = json.load(f)\n",
    "\n",
    "print(f\"âœ… Loaded {len(poems)} Telugu poems\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nðŸ“ Sample poem:\")\n",
    "print(\"-\" * 50)\n",
    "print(poems[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "from collections import Counter\n",
    "\n",
    "styles = Counter([p.get('style', 'unknown') for p in poems])\n",
    "authors = Counter([p.get('author', 'unknown') for p in poems])\n",
    "\n",
    "print(\"ðŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Total poems: {len(poems)}\")\n",
    "print(f\"\\n   Styles:\")\n",
    "for style, count in styles.most_common(10):\n",
    "    print(f\"      {style}: {count}\")\n",
    "print(f\"\\n   Top Authors:\")\n",
    "for author, count in authors.most_common(5):\n",
    "    print(f\"      {author}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98369c",
   "metadata": {},
   "source": [
    "## Step 3: Create Model V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf26184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.telugu_generator_v2 import create_telugu_generator_v2\n",
    "\n",
    "# Create improved model\n",
    "# Options: 'mbert' (recommended), 'distilmbert' (lighter), 'xlm-roberta' (heavier)\n",
    "MODEL_TYPE = 'mbert'\n",
    "FREEZE_ENCODER = False  # False = fine-tune encoder (better quality, slower)\n",
    "\n",
    "print(f\"ðŸ§  Creating Telugu Generator V2 ({MODEL_TYPE})...\")\n",
    "model = create_telugu_generator_v2(\n",
    "    model_type=MODEL_TYPE,\n",
    "    freeze_encoder=FREEZE_ENCODER\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total, trainable = model.count_parameters()\n",
    "print(f\"\\nðŸ“Š Model Statistics:\")\n",
    "print(f\"   Total parameters: {total:,}\")\n",
    "print(f\"   Trainable: {trainable:,} ({100*trainable/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60251c05",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.telugu_cleaner import TeluguTextCleaner\n",
    "import random\n",
    "\n",
    "class TeluguPoemDatasetV2(Dataset):\n",
    "    \"\"\"Improved dataset with augmentation and proper label creation.\"\"\"\n",
    "    \n",
    "    def __init__(self, poems, tokenizer, max_length=128, augment=True):\n",
    "        self.poems = poems\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.cleaner = TeluguTextCleaner()\n",
    "        self.data = self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        data = []\n",
    "        for poem in self.poems:\n",
    "            text = poem.get('text', '') if isinstance(poem, dict) else poem\n",
    "            text = self.cleaner.clean(text)\n",
    "            if len(text) > 10:\n",
    "                data.append(text)\n",
    "                # Add individual lines for more training data\n",
    "                lines = text.split('\\n')\n",
    "                for line in lines:\n",
    "                    if len(line.strip()) > 10:\n",
    "                        data.append(line.strip())\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        \n",
    "        # Simple augmentation: random word shuffling\n",
    "        if self.augment and random.random() < 0.15:\n",
    "            words = text.split()\n",
    "            if len(words) > 3:\n",
    "                i = random.randint(1, len(words)-2)\n",
    "                words[i], words[i+1] = words[i+1], words[i]\n",
    "                text = ' '.join(words)\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        # Create labels (same as input, pad tokens masked)\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "dataset = TeluguPoemDatasetV2(\n",
    "    poems, \n",
    "    model.tokenizer, \n",
    "    max_length=128,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"âœ… Dataset created:\")\n",
    "print(f\"   Total: {len(dataset)}\")\n",
    "print(f\"   Train: {len(train_dataset)}\")\n",
    "print(f\"   Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "BATCH_SIZE = 4  # Adjust based on your GPU memory\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"âœ… DataLoaders ready:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45020d81",
   "metadata": {},
   "source": [
    "## Step 5: Training with Anti-Repetition Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'epochs': 20,\n",
    "    'learning_rate': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 200,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'accumulation_steps': 4,  # Effective batch size = 16\n",
    "    'log_every': 50,\n",
    "    'eval_every': 2,  # epochs\n",
    "    'save_every': 5,  # epochs\n",
    "    # Anti-repetition weights\n",
    "    'diversity_weight': 0.1,\n",
    "    'repetition_weight': 0.2\n",
    "}\n",
    "\n",
    "print(\"ðŸ“‹ Training Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb90106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import math\n",
    "\n",
    "# Optimizer with weight decay\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_params = [\n",
    "    {'params': [p for n, p in model.named_parameters() \n",
    "                if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "     'weight_decay': CONFIG['weight_decay']},\n",
    "    {'params': [p for n, p in model.named_parameters() \n",
    "                if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_params, lr=CONFIG['learning_rate'])\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "total_steps = len(train_loader) * CONFIG['epochs'] // CONFIG['accumulation_steps']\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < CONFIG['warmup_steps']:\n",
    "        return CONFIG['learning_rate'] * (step + 1) / CONFIG['warmup_steps']\n",
    "    else:\n",
    "        progress = (step - CONFIG['warmup_steps']) / max(1, total_steps - CONFIG['warmup_steps'])\n",
    "        return CONFIG['learning_rate'] * (1 + math.cos(math.pi * progress)) / 2\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {CONFIG['warmup_steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom diversity loss to prevent repetition\n",
    "class DiversityLoss(nn.Module):\n",
    "    def __init__(self, weight=0.1):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        # Encourage diverse predictions by maximizing entropy\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=-1)\n",
    "        # We want some entropy (diversity) but not too much (quality)\n",
    "        # Target entropy around 2-3\n",
    "        target_entropy = 2.5\n",
    "        loss = F.relu(target_entropy - entropy.mean())\n",
    "        return self.weight * loss\n",
    "\n",
    "class RepetitionPenalty(nn.Module):\n",
    "    def __init__(self, weight=0.2, window=10):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.window = window\n",
    "    \n",
    "    def forward(self, logits, input_ids):\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        loss = torch.tensor(0.0, device=logits.device)\n",
    "        \n",
    "        for t in range(self.window, seq_len):\n",
    "            recent = input_ids[:, t-self.window:t]\n",
    "            probs = F.softmax(logits[:, t, :], dim=-1)\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                for token in recent[b].unique():\n",
    "                    if token != 0:  # Skip padding\n",
    "                        loss = loss + probs[b, token]\n",
    "        \n",
    "        return self.weight * loss / (batch_size * max(1, seq_len - self.window))\n",
    "\n",
    "diversity_loss_fn = DiversityLoss(CONFIG['diversity_weight'])\n",
    "repetition_loss_fn = RepetitionPenalty(CONFIG['repetition_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a7879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path('checkpoints/telugu_v2')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Test prompts for evaluation\n",
    "test_prompts = [\n",
    "    \"à°šà°‚à°¦à°®à°¾à°® à°°à°¾à°µà±‡\",\n",
    "    \"à°¤à±†à°²à±à°—à± à°­à°¾à°· à°®à°¨\",\n",
    "    \"à°…à°®à±à°® à°ªà±à°°à±‡à°®\"\n",
    "]\n",
    "\n",
    "# Tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_loss = float('inf')\n",
    "global_step = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ Starting Telugu Poem Training V2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress = tqdm(train_loader, desc=f\"Epoch {epoch}/{CONFIG['epochs']}\")\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs['loss']\n",
    "        if loss is None:\n",
    "            continue\n",
    "        \n",
    "        # Add anti-repetition losses\n",
    "        if outputs.get('logits') is not None:\n",
    "            loss = loss + diversity_loss_fn(outputs['logits'])\n",
    "            loss = loss + repetition_loss_fn(outputs['logits'], input_ids)\n",
    "        \n",
    "        # Scale for accumulation\n",
    "        loss = loss / CONFIG['accumulation_steps']\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        if (batch_idx + 1) % CONFIG['accumulation_steps'] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "            \n",
    "            # Update LR\n",
    "            lr = get_lr(global_step)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        \n",
    "        epoch_loss += loss.item() * CONFIG['accumulation_steps']\n",
    "        num_batches += 1\n",
    "        \n",
    "        progress.set_postfix({'loss': f'{loss.item() * CONFIG[\"accumulation_steps\"]:.4f}', 'lr': f'{get_lr(global_step):.2e}'})\n",
    "    \n",
    "    avg_train_loss = epoch_loss / num_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"\\nðŸ“Š Epoch {epoch} | Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    if epoch % CONFIG['eval_every'] == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        num_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                if outputs['loss'] is not None:\n",
    "                    val_loss += outputs['loss'].item()\n",
    "                    num_val += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / max(1, num_val)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"   Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Test generation\n",
    "    if epoch % CONFIG['eval_every'] == 0:\n",
    "        print(\"\\nðŸ“ Generation Test:\")\n",
    "        model.eval()\n",
    "        for prompt in test_prompts:\n",
    "            try:\n",
    "                generated = model.generate(\n",
    "                    prompt,\n",
    "                    max_length=40,\n",
    "                    temperature=0.8,\n",
    "                    repetition_penalty=1.5,\n",
    "                    no_repeat_ngram_size=3\n",
    "                )\n",
    "                print(f\"   {prompt} â†’ {generated[:80]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Error: {e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_train_loss < best_loss:\n",
    "        best_loss = avg_train_loss\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': best_loss\n",
    "        }, save_dir / 'best_model.pt')\n",
    "        print(f\"   â­ Best model saved!\")\n",
    "    \n",
    "    # Regular checkpoint\n",
    "    if epoch % CONFIG['save_every'] == 0:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': avg_train_loss\n",
    "        }, save_dir / f'checkpoint_epoch_{epoch}.pt')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Training Complete!\")\n",
    "print(f\"   Best Loss: {best_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ac0a9",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5131cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "if val_losses:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_losses, label='Val Loss', color='orange')\n",
    "    plt.xlabel('Evaluation Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf06dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive generation test\n",
    "model.eval()\n",
    "\n",
    "test_prompts = [\n",
    "    \"à°šà°‚à°¦à°®à°¾à°® à°°à°¾à°µà±‡\",\n",
    "    \"à°¤à±†à°²à±à°—à± à°­à°¾à°· à°®à°¨\",\n",
    "    \"à°…à°®à±à°® à°ªà±à°°à±‡à°® à°—à±Šà°ªà±à°ªà°¦à°¿\",\n",
    "    \"à°¨à°¾ à°¦à±‡à°¶à°‚ à°­à°¾à°°à°¤à°‚\",\n",
    "    \"à°ªà±‚à°²à± à°ªà±‚à°¯à°—\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“ Telugu Poem Generation Test\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test different sampling strategies\n",
    "strategies = [\n",
    "    {'name': 'Nucleus (p=0.9)', 'top_p': 0.9, 'top_k': 0, 'temp': 0.8},\n",
    "    {'name': 'Top-k (k=50)', 'top_p': 1.0, 'top_k': 50, 'temp': 0.8},\n",
    "    {'name': 'High Temp (1.2)', 'top_p': 0.95, 'top_k': 0, 'temp': 1.2}\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nðŸ”¹ Prompt: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        try:\n",
    "            generated = model.generate(\n",
    "                prompt,\n",
    "                max_length=50,\n",
    "                temperature=strategy['temp'],\n",
    "                top_k=strategy['top_k'],\n",
    "                top_p=strategy['top_p'],\n",
    "                repetition_penalty=1.5,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "            print(f\"   [{strategy['name']}]: {generated}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   [{strategy['name']}]: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6496c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze repetition in generated text\n",
    "def analyze_repetition(text):\n",
    "    \"\"\"Analyze repetition metrics in generated text.\"\"\"\n",
    "    words = text.split()\n",
    "    \n",
    "    if len(words) < 2:\n",
    "        return {'consecutive_rep': 0, 'unique_ratio': 1.0, 'bigram_rep': 0}\n",
    "    \n",
    "    # Consecutive repetition\n",
    "    consecutive = sum(1 for i in range(1, len(words)) if words[i] == words[i-1])\n",
    "    \n",
    "    # Unique word ratio\n",
    "    unique_ratio = len(set(words)) / len(words)\n",
    "    \n",
    "    # Bigram repetition\n",
    "    bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "    bigram_rep = 1 - len(set(bigrams)) / max(1, len(bigrams))\n",
    "    \n",
    "    return {\n",
    "        'consecutive_rep': consecutive / len(words),\n",
    "        'unique_ratio': unique_ratio,\n",
    "        'bigram_rep': bigram_rep\n",
    "    }\n",
    "\n",
    "# Generate samples and analyze\n",
    "print(\"\\nðŸ“Š Repetition Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts[:3]:\n",
    "    generated = model.generate(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        temperature=0.8,\n",
    "        repetition_penalty=1.5,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    \n",
    "    metrics = analyze_repetition(generated)\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated[:100]}...\")\n",
    "    print(f\"Metrics:\")\n",
    "    print(f\"  - Consecutive repetition: {metrics['consecutive_rep']:.2%}\")\n",
    "    print(f\"  - Unique word ratio: {metrics['unique_ratio']:.2%}\")\n",
    "    print(f\"  - Bigram repetition: {metrics['bigram_rep']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a95c4",
   "metadata": {},
   "source": [
    "## Step 7: Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model with config\n",
    "final_save = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'model_type': MODEL_TYPE,\n",
    "    'best_loss': best_loss,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses\n",
    "}\n",
    "\n",
    "torch.save(final_save, save_dir / 'final_model.pt')\n",
    "\n",
    "print(f\"âœ… Final model saved to: {save_dir / 'final_model.pt'}\")\n",
    "print(f\"\\nðŸ“‹ To load the model:\")\n",
    "print(f\"   checkpoint = torch.load('{save_dir}/final_model.pt')\")\n",
    "print(f\"   model.load_state_dict(checkpoint['model_state_dict'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f02389",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Tips for Better Generation\n",
    "\n",
    "If you're still seeing repetition:\n",
    "\n",
    "1. **Increase `repetition_penalty`** (try 1.5 - 2.0)\n",
    "2. **Increase `no_repeat_ngram_size`** (try 4-5)\n",
    "3. **Lower `temperature`** for more focused output (0.6-0.8)\n",
    "4. **Use nucleus sampling** with `top_p=0.9`\n",
    "5. **Train for more epochs** (30-50)\n",
    "6. **Use larger dataset** if possible\n",
    "7. **Unfreeze encoder** for better Telugu understanding"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

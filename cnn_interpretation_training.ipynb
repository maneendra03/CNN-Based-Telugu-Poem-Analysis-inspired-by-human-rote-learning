{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ CNN-Based Telugu Poem Interpretation - Training\n",
                "\n",
                "**Train the CNN+RNN model for Telugu poem analysis and interpretation**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Setup & Clone Project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone from GitHub\n",
                "!git clone https://github.com/maneendra03/CNN-Based-Telugu-Poem-Analysis-inspired-by-human-rote-learning.git /content/project\n",
                "%cd /content/project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install torch torchvision torchaudio\n",
                "!pip install tqdm pyyaml"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Load Telugu Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import sys\n",
                "sys.path.insert(0, '/content/project')\n",
                "\n",
                "from src.data.data_loader import PoemDataLoader\n",
                "from src.preprocessing.tokenizer import PoemTokenizer\n",
                "\n",
                "# Load poems\n",
                "with open('data/processed/telugu_poems.json', 'r', encoding='utf-8') as f:\n",
                "    poems = json.load(f)\n",
                "\n",
                "print(f\"‚úÖ Loaded {len(poems)} Telugu poems\")\n",
                "print(f\"Sample: {poems[0]['text'][:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Create DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import Dataset, DataLoader\n",
                "from src.preprocessing.telugu_cleaner import TeluguTextCleaner\n",
                "\n",
                "class TeluguPoemDataset(Dataset):\n",
                "    def __init__(self, poems, tokenizer, max_length=100):\n",
                "        self.poems = poems\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "        self.cleaner = TeluguTextCleaner()\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.poems)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        poem = self.poems[idx]\n",
                "        text = poem.get('text', '') if isinstance(poem, dict) else poem\n",
                "        text = self.cleaner.clean(text)\n",
                "        \n",
                "        # Encode\n",
                "        tokens = self.tokenizer.encode(text)\n",
                "        \n",
                "        # Pad/truncate\n",
                "        if len(tokens) > self.max_length:\n",
                "            tokens = tokens[:self.max_length]\n",
                "        else:\n",
                "            tokens = tokens + [0] * (self.max_length - len(tokens))\n",
                "        \n",
                "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)\n",
                "        target_ids = torch.tensor(tokens[1:], dtype=torch.long)\n",
                "        \n",
                "        return {\n",
                "            'input_ids': input_ids,\n",
                "            'target_ids': target_ids\n",
                "        }\n",
                "\n",
                "# Create tokenizer\n",
                "tokenizer = PoemTokenizer(min_freq=1)\n",
                "tokenizer.fit([p['text'] for p in poems])\n",
                "\n",
                "print(f\"‚úÖ Tokenizer: vocab_size={tokenizer.word_vocab_size}\")\n",
                "\n",
                "# Create datasets\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "train_poems, test_poems = train_test_split(poems, test_size=0.2, random_state=42)\n",
                "train_poems, val_poems = train_test_split(train_poems, test_size=0.1, random_state=42)\n",
                "\n",
                "train_dataset = TeluguPoemDataset(train_poems, tokenizer, max_length=100)\n",
                "val_dataset = TeluguPoemDataset(val_poems, tokenizer, max_length=100)\n",
                "test_dataset = TeluguPoemDataset(test_poems, tokenizer, max_length=100)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
                "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
                "\n",
                "print(f\"‚úÖ Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Initialize CNN Interpretation Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.poem_learner import PoemLearner\n",
                "\n",
                "# Create CNN-based interpretation model\n",
                "model = PoemLearner(\n",
                "    vocab_size=tokenizer.word_vocab_size,\n",
                "    embedding_dim=256,\n",
                "    hidden_dim=512\n",
                ")\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = model.to(device)\n",
                "\n",
                "# Count parameters\n",
                "total = sum(p.numel() for p in model.parameters())\n",
                "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "\n",
                "print(f\"‚úÖ CNN Interpretation Model\")\n",
                "print(f\"   Total params: {total:,}\")\n",
                "print(f\"   Trainable: {trainable:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Train CNN Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.optim import Adam\n",
                "from tqdm import tqdm\n",
                "from pathlib import Path\n",
                "\n",
                "# Training config\n",
                "CONFIG = {\n",
                "    'epochs': 50,\n",
                "    'learning_rate': 1e-3,\n",
                "    'save_every': 5\n",
                "}\n",
                "\n",
                "optimizer = Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
                "\n",
                "# Create checkpoint dir\n",
                "checkpoint_dir = Path('/content/project/checkpoints/interpretation')\n",
                "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(\"üöÄ Training CNN Interpretation Model...\")\n",
                "print(f\"   Epochs: {CONFIG['epochs']}\")\n",
                "print(f\"   Batches: {len(train_loader)}\")\n",
                "\n",
                "best_loss = float('inf')\n",
                "\n",
                "for epoch in range(CONFIG['epochs']):\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    \n",
                "    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
                "    for batch in progress:\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        target_ids = batch['target_ids'].to(device)\n",
                "        \n",
                "        # Forward pass\n",
                "        outputs = model(input_ids, target_ids)\n",
                "        \n",
                "        # Loss is in the output dict\n",
                "        if 'loss' in outputs and outputs['loss'] is not None:\n",
                "            loss = outputs['loss']\n",
                "        else:\n",
                "            # Calculate loss manually\n",
                "            from torch.nn import CrossEntropyLoss\n",
                "            loss_fn = CrossEntropyLoss(ignore_index=0)\n",
                "            loss = loss_fn(outputs['logits'].view(-1, outputs['logits'].size(-1)), target_ids.view(-1))\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optimizer.step()\n",
                "        \n",
                "        epoch_loss += loss.item()\n",
                "        progress.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    \n",
                "    avg_loss = epoch_loss / len(train_loader)\n",
                "    print(f\"\\nüìä Epoch {epoch+1} | Loss: {avg_loss:.4f}\")\n",
                "    \n",
                "    # Save checkpoint\n",
                "    if (epoch + 1) % CONFIG['save_every'] == 0 or avg_loss < best_loss:\n",
                "        if avg_loss < best_loss:\n",
                "            best_loss = avg_loss\n",
                "            save_path = checkpoint_dir / 'best_cnn_interpretation.pt'\n",
                "        else:\n",
                "            save_path = checkpoint_dir / f'cnn_epoch_{epoch+1}.pt'\n",
                "        \n",
                "        torch.save({\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'optimizer_state_dict': optimizer.state_dict(),\n",
                "            'epoch': epoch,\n",
                "            'loss': avg_loss,\n",
                "            'vocab_size': tokenizer.word_vocab_size\n",
                "        }, save_path)\n",
                "        print(f\"üíæ Saved: {save_path}\")\n",
                "\n",
                "print(\"\\n‚úÖ Training Complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Test Interpretation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test poem interpretation\n",
                "model.eval()\n",
                "\n",
                "test_poems = [\n",
                "    \"‡∞ö‡∞Ç‡∞¶‡∞Æ‡∞æ‡∞Æ ‡∞∞‡∞æ‡∞µ‡±á ‡∞ö‡∞æ‡∞≤ ‡∞¨‡∞æ‡∞ó‡±Å‡∞Ç‡∞¶‡±á\",\n",
                "    \"‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞≠‡∞æ‡∞∑ ‡∞Æ‡∞ß‡±Å‡∞∞‡∞Æ‡±à‡∞®‡∞¶‡∞ø\",\n",
                "    \"‡∞Ö‡∞Æ‡±ç‡∞Æ ‡∞™‡±ç‡∞∞‡±á‡∞Æ ‡∞Ö‡∞Æ‡±É‡∞§‡∞Æ‡∞Ø‡∞Ç\"\n",
                "]\n",
                "\n",
                "print(\"üìù Telugu Poem Interpretation Test\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "for poem_text in test_poems:\n",
                "    print(f\"\\nüîπ Poem: {poem_text}\")\n",
                "    \n",
                "    # Encode\n",
                "    tokens = tokenizer.encode(poem_text)\n",
                "    if len(tokens) > 50:\n",
                "        tokens = tokens[:50]\n",
                "    else:\n",
                "        tokens = tokens + [0] * (50 - len(tokens))\n",
                "    \n",
                "    input_ids = torch.tensor([tokens[:-1]]).to(device)\n",
                "    target_ids = torch.tensor([tokens[1:]]).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        output = model(input_ids, target_ids)\n",
                "    \n",
                "    print(f\"   ‚úÖ Interpreted successfully\")\n",
                "    print(f\"   Features: {output['poem_representation'].shape}\")\n",
                "    print(f\"   Logits: {output['logits'].shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Save Final Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final model\n",
                "final_path = checkpoint_dir / 'final_cnn_interpretation.pt'\n",
                "\n",
                "torch.save({\n",
                "    'model_state_dict': model.state_dict(),\n",
                "    'config': CONFIG,\n",
                "    'vocab_size': tokenizer.word_vocab_size,\n",
                "    'best_loss': best_loss\n",
                "}, final_path)\n",
                "\n",
                "print(f\"‚úÖ Final model saved: {final_path}\")\n",
                "print(f\"\\nüìÅ Download from: /content/project/checkpoints/interpretation/\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
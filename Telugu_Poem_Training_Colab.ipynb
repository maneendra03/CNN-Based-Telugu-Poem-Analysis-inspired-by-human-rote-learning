{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6739f9eb",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory in Drive\n",
    "import os\n",
    "DRIVE_CHECKPOINT_DIR = '/content/drive/MyDrive/telugu_poem_checkpoints'\n",
    "os.makedirs(DRIVE_CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"‚úì Checkpoints will be saved to: {DRIVE_CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73382a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "# ‚ö†Ô∏è IMPORTANT: Replace with your actual GitHub repository URL\n",
    "GITHUB_REPO = \"https://github.com/maneendra03/CNN-Based-Telugu-Poem-Analysis-inspired-by-human-rote-learning.git\"  # <-- CHANGE THIS!\n",
    "\n",
    "import os\n",
    "\n",
    "# Remove existing directory if present\n",
    "!rm -rf /content/telugu-poem-generator\n",
    "\n",
    "# Clone repository\n",
    "!git clone {GITHUB_REPO} /content/telugu-poem-generator\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/content/telugu-poem-generator')\n",
    "print(f\"\\n‚úì Working directory: {os.getcwd()}\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets torch tqdm pyyaml\n",
    "\n",
    "# Install any additional requirements\n",
    "!pip install -q -r requirements.txt 2>/dev/null || echo \"No requirements.txt or already satisfied\"\n",
    "\n",
    "print(\"\\n‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa697ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify project structure\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "required_files = [\n",
    "    'src/models/enhanced_generator.py',\n",
    "    'src/training/enhanced_trainer.py',\n",
    "    'src/preprocessing/advanced_preprocessor.py',\n",
    "    'src/interpretation/poem_interpreter.py',\n",
    "    'data/processed/telugu_train.json',\n",
    "    'data/processed/telugu_val.json',\n",
    "]\n",
    "\n",
    "print(\"üìÅ Checking project structure...\")\n",
    "all_present = True\n",
    "for f in required_files:\n",
    "    exists = os.path.exists(f)\n",
    "    status = '‚úì' if exists else '‚úó'\n",
    "    print(f\"  {status} {f}\")\n",
    "    if not exists:\n",
    "        all_present = False\n",
    "\n",
    "if all_present:\n",
    "    print(\"\\n‚úÖ All required files present!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Some files missing! Check your repository.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2d0ef",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load and Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load and check datasets\n",
    "data_files = {\n",
    "    'train': 'data/processed/telugu_train.json',\n",
    "    'val': 'data/processed/telugu_val.json',\n",
    "    'test': 'data/processed/telugu_test.json'\n",
    "}\n",
    "\n",
    "print(\"üìä Dataset Statistics:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total_poems = 0\n",
    "for split, path in data_files.items():\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, dict) and 'poems' in data:\n",
    "            count = len(data['poems'])\n",
    "        else:\n",
    "            count = len(data)\n",
    "        \n",
    "        total_poems += count\n",
    "        print(f\"  {split:>6}: {count:,} poems\")\n",
    "    else:\n",
    "        print(f\"  {split:>6}: NOT FOUND\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"  {'Total':>6}: {total_poems:,} poems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample poems\n",
    "with open('data/processed/telugu_train.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "if isinstance(train_data, dict) and 'poems' in train_data:\n",
    "    poems = train_data['poems']\n",
    "else:\n",
    "    poems = train_data\n",
    "\n",
    "print(\"üìú Sample Poems:\")\n",
    "print(\"=\"*60)\n",
    "for i, poem in enumerate(poems[:3]):\n",
    "    if isinstance(poem, dict):\n",
    "        text = poem.get('text', poem.get('content', str(poem)))\n",
    "    else:\n",
    "        text = str(poem)\n",
    "    print(f\"\\n[{i+1}] {text[:200]}{'...' if len(text) > 200 else ''}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d682fc77",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1890dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/telugu-poem-generator')\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "from src.models.enhanced_generator import (\n",
    "    TeluguPoemGeneratorV3,\n",
    "    GenerationConfig,\n",
    "    create_enhanced_generator\n",
    ")\n",
    "\n",
    "print(\"üöÄ Creating Telugu Poem Generator V3...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create model - use indic-bert for better Telugu support\n",
    "model = create_enhanced_generator(\n",
    "    model_type='mbert',  # Options: 'indic-bert', 'mbert', 'xlm-roberta'\n",
    "    freeze_encoder=True\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params, trainable_params = model.count_parameters()\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Total Parameters: {total_params:,}\")\n",
    "print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"   Frozen Parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3607abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation before training (untrained model)\n",
    "print(\"üé≠ Testing Generation (Before Training):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "config = GenerationConfig(\n",
    "    max_length=80,\n",
    "    min_length=20,\n",
    "    temperature=0.85,\n",
    "    repetition_penalty=1.8\n",
    ")\n",
    "\n",
    "test_prompts = [\"‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞≠‡∞æ‡∞∑\", \"‡∞Ö‡∞Æ‡±ç‡∞Æ ‡∞™‡±ç‡∞∞‡±á‡∞Æ\"]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    output = model.generate(prompt, config)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {output[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f59a6",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64271ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.enhanced_trainer import (\n",
    "    EnhancedTrainer,\n",
    "    TrainingConfig,\n",
    "    TeluguPoemDataset\n",
    ")\n",
    "\n",
    "# Training Configuration\n",
    "# Adjust these based on your GPU memory\n",
    "\n",
    "EPOCHS = 100  # Full training\n",
    "BATCH_SIZE = 16  # Reduce if OOM error\n",
    "LEARNING_RATE = 5e-4\n",
    "MAX_LENGTH = 128  # Sequence length\n",
    "\n",
    "config = TrainingConfig(\n",
    "    # Model\n",
    "    model_name='bert-base-multilingual-cased',\n",
    "    freeze_encoder=True,\n",
    "    \n",
    "    # Training\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=2,  # Effective batch = 32\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler_type='cosine',\n",
    "    \n",
    "    # Mixed precision (faster on GPU)\n",
    "    use_amp=torch.cuda.is_available(),\n",
    "    \n",
    "    # Regularization\n",
    "    label_smoothing=0.1,\n",
    "    dropout=0.2,\n",
    "    \n",
    "    # Loss weights\n",
    "    coverage_weight=0.1,\n",
    "    repetition_loss_weight=0.2,\n",
    "    \n",
    "    # Validation\n",
    "    val_every_n_steps=500,\n",
    "    patience=15,\n",
    "    min_delta=1e-4,\n",
    "    \n",
    "    # Checkpoints - save to Google Drive\n",
    "    checkpoint_dir=DRIVE_CHECKPOINT_DIR,\n",
    "    save_every_n_steps=1000,\n",
    "    max_checkpoints=5,\n",
    "    \n",
    "    # Data\n",
    "    max_length=MAX_LENGTH,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Training Configuration:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in vars(config).items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"üìö Loading Datasets...\")\n",
    "\n",
    "train_dataset = TeluguPoemDataset(\n",
    "    'data/processed/telugu_train.json',\n",
    "    model.tokenizer,\n",
    "    max_length=config.max_length\n",
    ")\n",
    "\n",
    "val_dataset = TeluguPoemDataset(\n",
    "    'data/processed/telugu_val.json',\n",
    "    model.tokenizer,\n",
    "    max_length=config.max_length\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Train samples: {len(train_dataset):,}\")\n",
    "print(f\"‚úì Validation samples: {len(val_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "print(\"üèãÔ∏è Creating Trainer...\")\n",
    "\n",
    "trainer = EnhancedTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b98d45",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Run Training (100 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "print(\"üöÄ Starting Training...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training for {EPOCHS} epochs\")\n",
    "print(f\"Checkpoints will be saved to: {DRIVE_CHECKPOINT_DIR}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    results = trainer.train()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    hours = int(training_time // 3600)\n",
    "    minutes = int((training_time % 3600) // 60)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Time: {hours}h {minutes}m\")\n",
    "    print(f\"Best Val Loss: {results.get('best_val_loss', 'N/A')}\")\n",
    "    print(f\"Final Train Loss: {results['train_losses'][-1]:.4f}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted! Checkpoints are saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90837e34",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03acc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model from checkpoints\n",
    "import glob\n",
    "\n",
    "best_model_path = os.path.join(DRIVE_CHECKPOINT_DIR, 'best_model.pt')\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"üì• Loading best model from: {best_model_path}\")\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"   Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"   Loss: {checkpoint.get('loss', 'N/A')}\")\n",
    "    print(\"‚úì Best model loaded!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Best model not found, using current model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation after training\n",
    "print(\"üé≠ Testing Generation (After Training):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "gen_config = GenerationConfig(\n",
    "    max_length=120,\n",
    "    min_length=30,\n",
    "    temperature=0.85,\n",
    "    top_k=50,\n",
    "    top_p=0.92,\n",
    "    repetition_penalty=1.8,\n",
    "    no_repeat_ngram_size=4,\n",
    "    diversity_penalty=0.5\n",
    ")\n",
    "\n",
    "test_prompts = [\n",
    "    \"‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞≠‡∞æ‡∞∑\",\n",
    "    \"‡∞Ö‡∞Æ‡±ç‡∞Æ ‡∞™‡±ç‡∞∞‡±á‡∞Æ\",\n",
    "    \"‡∞ß‡∞∞‡±ç‡∞Æ‡∞Ç ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ó‡∞Ç\",\n",
    "    \"‡∞µ‡∞ø‡∞¶‡±ç‡∞Ø ‡∞®‡±á‡∞∞‡±ç‡∞ö‡±Å‡∞ï‡±ã\",\n",
    "    \"‡∞∏‡±ç‡∞®‡±á‡∞π‡∞Ç ‡∞Æ‡∞Ç‡∞ö‡∞ø‡∞¶‡∞ø\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "for prompt in test_prompts:\n",
    "    output = model.generate(prompt, gen_config)\n",
    "    print(f\"\\nüìù Prompt: {prompt}\")\n",
    "    print(f\"üìú Generated:\\n{output}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9982a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test style-conditioned generation\n",
    "print(\"üé® Style-Conditioned Generation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "styles = ['vemana', 'sumati']\n",
    "prompt = \"‡∞®‡±Ä‡∞§‡∞ø ‡∞¨‡±ã‡∞ß‡∞®\"\n",
    "\n",
    "for style in styles:\n",
    "    output = model.generate_with_style(prompt, style=style, config=gen_config)\n",
    "    print(f\"\\nüè∑Ô∏è Style: {style}\")\n",
    "    print(f\"üìú Output:\\n{output}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test interpretation of generated poems\n",
    "from src.interpretation.poem_interpreter import TeluguPoemInterpreter\n",
    "\n",
    "interpreter = TeluguPoemInterpreter()\n",
    "\n",
    "print(\"üìä Interpretation Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate and analyze\n",
    "prompt = \"‡∞ß‡∞∞‡±ç‡∞Æ‡∞Ç ‡∞∏‡∞§‡±ç‡∞Ø‡∞Ç\"\n",
    "generated = model.generate(prompt, gen_config)\n",
    "\n",
    "print(f\"\\nüìù Generated Poem:\\n{generated}\")\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "\n",
    "interpretation = interpreter.interpret(generated)\n",
    "\n",
    "print(f\"\\nüé≠ Rasa (Emotion): {interpretation['rasa']['dominant']}\")\n",
    "print(f\"üìö Themes: {[t[0] for t in interpretation['themes']['primary'][:3]]}\")\n",
    "print(f\"üìñ ≈öatakam Style: {interpretation.get('satakam', 'Not detected')}\")\n",
    "print(f\"‚≠ê Quality Score: {interpretation['quality']['overall']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48d4b6",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ea352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model to Drive\n",
    "final_model_path = os.path.join(DRIVE_CHECKPOINT_DIR, 'final_model.pt')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': vars(config),\n",
    "    'vocab_size': model.vocab_size,\n",
    "    'hidden_dim': model.hidden_dim,\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"üíæ Final model saved to: {final_model_path}\")\n",
    "\n",
    "# List all checkpoints\n",
    "print(\"\\nüìÅ Saved Checkpoints:\")\n",
    "for f in sorted(glob.glob(os.path.join(DRIVE_CHECKPOINT_DIR, '*.pt'))):\n",
    "    size = os.path.getsize(f) / 1e6\n",
    "    print(f\"   {os.path.basename(f)}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f59856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training results\n",
    "import json\n",
    "\n",
    "results_path = os.path.join(DRIVE_CHECKPOINT_DIR, 'training_results.json')\n",
    "\n",
    "training_summary = {\n",
    "    'epochs': EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'train_losses': results.get('train_losses', []),\n",
    "    'val_losses': results.get('val_losses', []),\n",
    "    'best_val_loss': results.get('best_val_loss', None),\n",
    "}\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"üìä Training results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "if 'train_losses' in results and results['train_losses']:\n",
    "    ax.plot(results['train_losses'], label='Train Loss', alpha=0.8)\n",
    "if 'val_losses' in results and results['val_losses']:\n",
    "    ax.plot(results['val_losses'], label='Val Loss', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Telugu Poem Generator - Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DRIVE_CHECKPOINT_DIR, 'training_curve.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìà Training curve saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd8891",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31943bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the trained model\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Preparing model for download...\")\n",
    "\n",
    "# Create a smaller export version\n",
    "export_path = '/content/telugu_poem_model_trained.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': model.vocab_size,\n",
    "    'hidden_dim': model.hidden_dim,\n",
    "}, export_path)\n",
    "\n",
    "print(f\"Model size: {os.path.getsize(export_path) / 1e6:.1f} MB\")\n",
    "print(\"\\n‚¨áÔ∏è Click the download link below:\")\n",
    "\n",
    "files.download(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002b9ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Training Complete!\n",
    "\n",
    "Your trained model has been saved to:\n",
    "- **Google Drive**: `/content/drive/MyDrive/telugu_poem_checkpoints/`\n",
    "- **Best Model**: `best_model.pt`\n",
    "- **Final Model**: `final_model.pt`\n",
    "\n",
    "### Next Steps:\n",
    "1. Download the trained model to your local machine\n",
    "2. Copy checkpoints from Google Drive to your project\n",
    "3. Update your project to load the trained weights\n",
    "\n",
    "### To use the trained model locally:\n",
    "```python\n",
    "from src.models.enhanced_generator import create_enhanced_generator\n",
    "import torch\n",
    "\n",
    "model = create_enhanced_generator('mbert')\n",
    "checkpoint = torch.load('path/to/best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Generate poems\n",
    "output = model.generate(\"‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞≠‡∞æ‡∞∑\")\n",
    "print(output)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
